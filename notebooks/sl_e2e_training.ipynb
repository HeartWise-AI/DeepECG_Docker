{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdelfrates/DeepECG_Docker/deploy-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning SL model for AFIB at 2y PTB-XL lite dataset\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, \n",
    "    BinaryAUROC, \n",
    "    BinaryAveragePrecision\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Define hugging face token\n",
    "hugging_face_token = \"\" # Set your Hugging Face token here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    Seed everything for reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    seed (int): The seed to set for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 9992.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download model from Hugging Face\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"heartwise/EfficientNetV2_SL_Model_Tunable\", \n",
    "    local_dir=\".\", \n",
    "    repo_type=\"model\", \n",
    "    token=hugging_face_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientNetv2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model = torch.load(\"sl_model.h5\")\n",
    "\n",
    "for param in sl_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up new classifier and reset weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.AdaptiveAvgPool1d):\n",
    "        # AdaptiveAvgPool1d has no parameters to initialize\n",
    "        pass\n",
    "    elif isinstance(m, nn.Dropout):\n",
    "        # Dropout has no parameters to initialize\n",
    "        pass\n",
    "\n",
    "sl_model = sl_model.apply(reinitialize_weights)\n",
    "\n",
    "\n",
    "class NewClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewClassifier, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        #self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(640, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "sl_model.classifier = NewClassifier()\n",
    "for param in sl_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "sl_model = sl_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 21094.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download ptb-xl lite data from Hugging Face\n",
    "dataset_dir = snapshot_download(\n",
    "    repo_id=\"heartwise/PTB-XL_lite\", \n",
    "    local_dir=\"ptb-xl_lite\", \n",
    "    repo_type=\"dataset\", \n",
    "    token=hugging_face_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_X = np.squeeze(\n",
    "    np.load(os.path.join(dataset_dir, \"ptb_xl_afib_2y_train_subset.npy\")),\n",
    ")\n",
    "val_ds_X = np.squeeze(\n",
    "    np.load(os.path.join(dataset_dir, \"ptb_xl_afib_2y_val_subset.npy\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\n",
    "    os.path.join(dataset_dir, \"ptb_xl_afib_2y_train_subset_labels.parquet\")\n",
    ")\n",
    "val_df = pd.read_parquet(\n",
    "    os.path.join(dataset_dir, \"ptb_xl_afib_2y_val_subset_labels.parquet\")\n",
    ")\n",
    "\n",
    "train_ds_Y = train_df['label_2y'].astype(int).tolist()\n",
    "val_ds_Y = val_df['label_2y'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_params = {\n",
    "        'batch_size': 254,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory':True,\n",
    "        'multiprocessing_context': 'fork',\n",
    "        'persistent_workers': True\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "        'batch_size': 254,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory':True,\n",
    "        'multiprocessing_context': 'fork',\n",
    "        'persistent_workers': True\n",
    "    }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        x = np.swapaxes(x, 0, 1)\n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "train_set = CustomDataset(train_ds_X, train_ds_Y)\n",
    "dataloader = torch.utils.data.DataLoader(train_set, **train_params)\n",
    "\n",
    "val_set = CustomDataset(val_ds_X, val_ds_Y)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Binary Focal Loss for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Initialize the Binary Focal Loss function.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: Weighting factor for the rare class (usually the minority class), default is 0.25.\n",
    "        - gamma: Focusing parameter that adjusts the rate at which easy examples are down-weighted, default is 2.0.\n",
    "        - reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default is 'mean'.\n",
    "        \"\"\"\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the loss function.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs: Predicted logits (before applying sigmoid), shape (batch_size, 1).\n",
    "        - targets: Ground truth labels, shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "        - loss: Calculated focal loss.\n",
    "        \"\"\"\n",
    "        # Convert targets to float\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "\n",
    "        # Calculate binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n",
    "\n",
    "        # Calculate the modulating factor (1 - p_t)^gamma\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        modulating_factor = torch.pow(1 - p_t, self.gamma)\n",
    "\n",
    "        # Apply the alpha factor\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "\n",
    "        # Combine factors to compute the final focal loss\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "\n",
    "        # Apply reduction method\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Metrics and Training/Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model, \n",
    "    dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    scheduler, \n",
    "    device, \n",
    "    num_epochs=10, \n",
    "    patience=3\n",
    "    ):\n",
    "    # Initialize metrics\n",
    "    accuracy = BinaryAccuracy().to(device)\n",
    "    auroc = BinaryAUROC().to(device)\n",
    "    auprc = BinaryAveragePrecision().to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        accuracy.reset()\n",
    "        auroc.reset()\n",
    "        auprc.reset()\n",
    "\n",
    "        # Training loop\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs} (Training)', leave=False)\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # Squeeze to match the shape of labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "\n",
    "            # Update metrics\n",
    "            accuracy.update(outputs, labels)\n",
    "            auroc.update(outputs, labels)\n",
    "            auprc.update(outputs, labels)\n",
    "\n",
    "            # Update tqdm progress bar with aggregate metrics\n",
    "            progress_bar.set_postfix(\n",
    "                loss=avg_loss,\n",
    "                acc=accuracy.compute().item(),\n",
    "                auroc=auroc.compute().item(),\n",
    "                auprc=auprc.compute().item()\n",
    "            )\n",
    "\n",
    "        # Free GPU memory after each training epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        accuracy.reset()\n",
    "        auroc.reset()\n",
    "        auprc.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc=f'Epoch {epoch+1}/{num_epochs} (Validation)', leave=False)\n",
    "            for i, (inputs, labels) in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()  # Squeeze to match the shape of labels\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                avg_val_loss = val_running_loss / (i + 1)\n",
    "\n",
    "                # Update metrics\n",
    "                accuracy.update(outputs, labels)\n",
    "                auroc.update(outputs, labels)\n",
    "                auprc.update(outputs, labels)\n",
    "\n",
    "                # Update tqdm progress bar with validation metrics\n",
    "                progress_bar.set_postfix(\n",
    "                    val_loss=avg_val_loss,\n",
    "                    val_acc=accuracy.compute().item(),\n",
    "                    val_auroc=auroc.compute().item(),\n",
    "                    val_auprc=auprc.compute().item()\n",
    "                )\n",
    "\n",
    "        # Free GPU memory after each validation epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # End of epoch logging (including learning rate)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, LR: {current_lr}, Training Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")\n",
    "        print(f\"Validation Accuracy: {accuracy.compute().item()}\")\n",
    "        print(f\"Validation AUROC: {auroc.compute().item()}\")\n",
    "        print(f\"Validation AUPRC: {auprc.compute().item()}\")\n",
    "\n",
    "        # Check if the validation loss improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model's state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "        # Scheduler step with warm restarts\n",
    "        scheduler.step()\n",
    "\n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "\n",
    "    # Load the best model state (if needed later)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Free GPU memory at the end of training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, LR: 0.001, Training Loss: 0.10344375111162663, Validation Loss: 0.11459421087056398\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5074581503868103\n",
      "Validation AUPRC: 0.09703006595373154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, LR: 0.0009890738003669028, Training Loss: 0.0937123941257596, Validation Loss: 0.09033680986613035\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.49239927530288696\n",
      "Validation AUPRC: 0.09474300593137741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, LR: 0.0009567727288213003, Training Loss: 0.08461196254938841, Validation Loss: 0.07981687225401402\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.4524739384651184\n",
      "Validation AUPRC: 0.08631341904401779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, LR: 0.0009045084971874737, Training Loss: 0.08005208056420088, Validation Loss: 0.07234698999673128\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5254093408584595\n",
      "Validation AUPRC: 0.10624224692583084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, LR: 0.0008345653031794292, Training Loss: 0.07999465055763721, Validation Loss: 0.06326500046998262\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.49052098393440247\n",
      "Validation AUPRC: 0.09969422221183777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, LR: 0.00075, Training Loss: 0.07618012093007565, Validation Loss: 0.06475048419088125\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5584064722061157\n",
      "Validation AUPRC: 0.11620519310235977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, LR: 0.0006545084971874737, Training Loss: 0.07415025494992733, Validation Loss: 0.0537027376703918\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5676276683807373\n",
      "Validation AUPRC: 0.11436361074447632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, LR: 0.0005522642316338268, Training Loss: 0.07704869098961353, Validation Loss: 0.05789340892806649\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.4764443635940552\n",
      "Validation AUPRC: 0.09478125721216202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, LR: 0.0004477357683661734, Training Loss: 0.07466145697981119, Validation Loss: 0.05669789295643568\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.4423295259475708\n",
      "Validation AUPRC: 0.08700594305992126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, LR: 0.0003454915028125264, Training Loss: 0.07765408605337143, Validation Loss: 0.051485198084264994\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5043873190879822\n",
      "Validation AUPRC: 0.1074489951133728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, LR: 0.0002500000000000001, Training Loss: 0.07007570285350084, Validation Loss: 0.05107170855626464\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5336669683456421\n",
      "Validation AUPRC: 0.11195982992649078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, LR: 0.00016543469682057103, Training Loss: 0.07153486087918282, Validation Loss: 0.0515222093090415\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5469081401824951\n",
      "Validation AUPRC: 0.11658299714326859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, LR: 9.549150281252631e-05, Training Loss: 0.07045465614646673, Validation Loss: 0.05142587190493941\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5388046503067017\n",
      "Validation AUPRC: 0.11704397946596146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, LR: 4.322727117869951e-05, Training Loss: 0.06976697873324156, Validation Loss: 0.0508447727188468\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.540520191192627\n",
      "Validation AUPRC: 0.1183457002043724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, LR: 1.0926199633097156e-05, Training Loss: 0.0693838307633996, Validation Loss: 0.05059600621461868\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5434988141059875\n",
      "Validation AUPRC: 0.11919495463371277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, LR: 0.0, Training Loss: 0.06935949623584747, Validation Loss: 0.050081104040145874\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.544285237789154\n",
      "Validation AUPRC: 0.1192476898431778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, LR: 1.0926199633097156e-05, Training Loss: 0.0691316407173872, Validation Loss: 0.04996328288689256\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5506328344345093\n",
      "Validation AUPRC: 0.12080682814121246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, LR: 4.322727117869957e-05, Training Loss: 0.06998142600059509, Validation Loss: 0.04976402409374714\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5452473759651184\n",
      "Validation AUPRC: 0.11964261531829834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, LR: 9.549150281252622e-05, Training Loss: 0.06947323121130466, Validation Loss: 0.049510282929986715\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5461807250976562\n",
      "Validation AUPRC: 0.11992952227592468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, LR: 0.00016543469682057078, Training Loss: 0.06908863130956888, Validation Loss: 0.04859771579504013\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5422730445861816\n",
      "Validation AUPRC: 0.1182282343506813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, LR: 0.0002499999999999998, Training Loss: 0.06975671742111444, Validation Loss: 0.04988825926557183\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5468634366989136\n",
      "Validation AUPRC: 0.1177796721458435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, LR: 0.0003454915028125263, Training Loss: 0.0697169853374362, Validation Loss: 0.049544093664735556\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.547191858291626\n",
      "Validation AUPRC: 0.11371799558401108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, LR: 0.000447735768366173, Training Loss: 0.07251705881208181, Validation Loss: 0.051039488054811954\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5478098392486572\n",
      "Validation AUPRC: 0.12066294997930527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, LR: 0.0005522642316338266, Training Loss: 0.07043930422514677, Validation Loss: 0.04989225836470723\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.5491824746131897\n",
      "Validation AUPRC: 0.122047558426857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, LR: 0.0006545084971874736, Training Loss: 0.07332280557602644, Validation Loss: 0.055592084769159555\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.49423858523368835\n",
      "Validation AUPRC: 0.10547289252281189\n",
      "Early stopping after 25 epochs due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, sl_model.parameters()), lr=1e-3)\n",
    "criterion = BinaryFocalLoss(gamma=2)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=0)\n",
    "\n",
    "e2e_sl_model_afib_2y = train_and_evaluate_model(\n",
    "    sl_model, \n",
    "    dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    scheduler, \n",
    "    device, \n",
    "    num_epochs=50, \n",
    "    patience=5\n",
    ")\n",
    "\n",
    "\n",
    "torch.save(e2e_sl_model_afib_2y, \"e2e_sl_model_afib_2y.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
