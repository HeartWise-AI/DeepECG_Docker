{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning SL model for AFIB at 2y PTB-XL lite dataset\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, \n",
    "    BinaryAUROC, \n",
    "    BinaryAveragePrecision\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Define hugging face token\n",
    "hugging_face_token = \"\" # Set your Hugging Face token here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    Seed everything for reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    seed (int): The seed to set for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 33961.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download model from Hugging Face\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"heartwise/EfficientNetV2_SL_Model_Tunable\", \n",
    "    local_dir=\".\", \n",
    "    repo_type=\"model\", \n",
    "    token=hugging_face_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientNetv2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from local directory\n",
    "sl_model = torch.load(\"sl_model.h5\")\n",
    "\n",
    "for param in sl_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewClassifier(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(NewClassifier, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.fc1 = nn.Linear(640, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model.classifier = NewClassifier(out_dim=1)\n",
    "for param in sl_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model = sl_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 56807.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download ptb-xl lite data from Hugging Face\n",
    "dataset_dir = snapshot_download(\n",
    "    repo_id=\"heartwise/PTB-XL_lite\", \n",
    "    local_dir=\"ptb-xl_lite\", \n",
    "    repo_type=\"dataset\", \n",
    "    token=hugging_face_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_X = np.squeeze(\n",
    "    np.load(os.path.join(dataset_dir, \"ptb_xl_afib_2y_train_subset.npy\")),\n",
    ")\n",
    "val_ds_X = np.squeeze(\n",
    "    np.load(os.path.join(dataset_dir, \"ptb_xl_afib_2y_val_subset.npy\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\n",
    "    os.path.join(dataset_dir, \"ptb_xl_afib_2y_train_subset_labels.parquet\")\n",
    ")\n",
    "val_df = pd.read_parquet(\n",
    "    os.path.join(dataset_dir, \"ptb_xl_afib_2y_val_subset_labels.parquet\")\n",
    ")\n",
    "\n",
    "train_ds_Y = train_df['label_2y'].astype(int).tolist()\n",
    "val_ds_Y = val_df['label_2y'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "        'batch_size': 254,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory':True,\n",
    "        'multiprocessing_context': 'fork',\n",
    "        'persistent_workers': True\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "        'batch_size': 254,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory':True,\n",
    "        'multiprocessing_context': 'fork',\n",
    "        'persistent_workers': True\n",
    "    }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        x = np.swapaxes(x, 0, 1)\n",
    "        \n",
    "        # Convert numpy arrays to torch tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "train_set = CustomDataset(train_ds_X, train_ds_Y)\n",
    "dataloader = torch.utils.data.DataLoader(train_set, **train_params)\n",
    "\n",
    "val_set = CustomDataset(val_ds_X, val_ds_Y)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Binary Focal Loss for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Initialize the Binary Focal Loss function.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: Weighting factor for the rare class (usually the minority class), default is 0.25.\n",
    "        - gamma: Focusing parameter that adjusts the rate at which easy examples are down-weighted, default is 2.0.\n",
    "        - reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default is 'mean'.\n",
    "        \"\"\"\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the loss function.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs: Predicted logits (before applying sigmoid), shape (batch_size, 1).\n",
    "        - targets: Ground truth labels, shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "        - loss: Calculated focal loss.\n",
    "        \"\"\"\n",
    "        # Convert targets to float\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "\n",
    "        # Calculate binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n",
    "\n",
    "        # Calculate the modulating factor (1 - p_t)^gamma\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        modulating_factor = torch.pow(1 - p_t, self.gamma)\n",
    "\n",
    "        # Apply the alpha factor\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "\n",
    "        # Combine factors to compute the final focal loss\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "\n",
    "        # Apply reduction method\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Metrics and Training/Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(\n",
    "    model, \n",
    "    dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    scheduler, \n",
    "    device, \n",
    "    num_epochs=10, \n",
    "    patience=3\n",
    "    ):\n",
    "    # Initialize metrics\n",
    "    accuracy = BinaryAccuracy().to(device)\n",
    "    auroc = BinaryAUROC().to(device)\n",
    "    auprc = BinaryAveragePrecision().to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        accuracy.reset()\n",
    "        auroc.reset()\n",
    "        auprc.reset()\n",
    "\n",
    "        # Training loop\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs} (Training)', leave=False)\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # Squeeze to match the shape of labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "\n",
    "            # Update metrics\n",
    "            accuracy.update(outputs, labels)\n",
    "            auroc.update(outputs, labels)\n",
    "            auprc.update(outputs, labels)\n",
    "\n",
    "            # Update tqdm progress bar with aggregate metrics\n",
    "            progress_bar.set_postfix(\n",
    "                loss=avg_loss,\n",
    "                acc=accuracy.compute().item(),\n",
    "                auroc=auroc.compute().item(),\n",
    "                auprc=auprc.compute().item()\n",
    "            )\n",
    "\n",
    "        # Free GPU memory after each training epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        accuracy.reset()\n",
    "        auroc.reset()\n",
    "        auprc.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc=f'Epoch {epoch+1}/{num_epochs} (Validation)', leave=False)\n",
    "            for i, (inputs, labels) in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()  # Squeeze to match the shape of labels\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                avg_val_loss = val_running_loss / (i + 1)\n",
    "\n",
    "                # Update metrics\n",
    "                accuracy.update(outputs, labels)\n",
    "                auroc.update(outputs, labels)\n",
    "                auprc.update(outputs, labels)\n",
    "\n",
    "                # Update tqdm progress bar with validation metrics\n",
    "                progress_bar.set_postfix(\n",
    "                    val_loss=avg_val_loss,\n",
    "                    val_acc=accuracy.compute().item(),\n",
    "                    val_auroc=auroc.compute().item(),\n",
    "                    val_auprc=auprc.compute().item()\n",
    "                )\n",
    "\n",
    "        # Free GPU memory after each validation epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # End of epoch logging (including learning rate)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, LR: {current_lr}, Training Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")\n",
    "        print(f\"Validation Accuracy: {accuracy.compute().item()}\")\n",
    "        print(f\"Validation AUROC: {auroc.compute().item()}\")\n",
    "        print(f\"Validation AUPRC: {auprc.compute().item()}\")\n",
    "\n",
    "        # Check if the validation loss improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model's state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "        # Scheduler step with warm restarts\n",
    "        scheduler.step()\n",
    "\n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "\n",
    "    # Load the best model state (if needed later)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Free GPU memory at the end of training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, LR: 0.01, Training Loss: 0.07342986296862364, Validation Loss: 0.042087603360414505\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.6281976103782654\n",
      "Validation AUPRC: 0.16620472073554993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, LR: 0.009890738003669028, Training Loss: 0.06786379870027304, Validation Loss: 0.04234182741492987\n",
      "Validation Accuracy: 0.9039999842643738\n",
      "Validation AUROC: 0.6419674158096313\n",
      "Validation AUPRC: 0.16286659240722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, LR: 0.009567727288213004, Training Loss: 0.06700187921524048, Validation Loss: 0.04757412290200591\n",
      "Validation Accuracy: 0.9024999737739563\n",
      "Validation AUROC: 0.6488580703735352\n",
      "Validation AUPRC: 0.16419777274131775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, LR: 0.009045084971874737, Training Loss: 0.0658906614407897, Validation Loss: 0.043100038543343544\n",
      "Validation Accuracy: 0.9024999737739563\n",
      "Validation AUROC: 0.6444391012191772\n",
      "Validation AUPRC: 0.16409432888031006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, LR: 0.008345653031794291, Training Loss: 0.06562896724790335, Validation Loss: 0.046420552767813206\n",
      "Validation Accuracy: 0.9010000228881836\n",
      "Validation AUROC: 0.6401928663253784\n",
      "Validation AUPRC: 0.15866629779338837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, LR: 0.0075, Training Loss: 0.06572936661541462, Validation Loss: 0.044125021900981665\n",
      "Validation Accuracy: 0.9024999737739563\n",
      "Validation AUROC: 0.63759446144104\n",
      "Validation AUPRC: 0.16052564978599548\n",
      "Early stopping after 6 epochs due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, sl_model.parameters()), lr=1e-2)\n",
    "criterion = BinaryFocalLoss(gamma=2)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=0)\n",
    "\n",
    "fine_tuned_sl_model_afib_2y = train_and_evaluate_model(\n",
    "    sl_model, \n",
    "    dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    scheduler, \n",
    "    device, \n",
    "    num_epochs=50, \n",
    "    patience=5\n",
    ")\n",
    "\n",
    "\n",
    "torch.save(fine_tuned_sl_model_afib_2y, \"fine_tuned_sl_model_afib_2y.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
